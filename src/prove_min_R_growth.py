#!/usr/bin/env python3
"""
GOAL: Prove min_cone R ~ N^Î´ for some Î´ > 0

Key insight from previous analysis:
- (1+Î±)/(1+Î²) â‰ˆ 0.51 is STABLE
- But min_R / R_uniform decreases (uniform not optimal)
- Still, min_R ~ N^{1.09} GROWS

Strategy: Prove lower bound on min_R directly.
"""

import numpy as np
from scipy.optimize import minimize
from scipy.linalg import eigh
import warnings
warnings.filterwarnings('ignore')

def get_twins(X_max):
    """Get twin primes up to X_max"""
    sieve = np.ones(X_max + 3, dtype=bool)
    sieve[0] = sieve[1] = False
    for i in range(2, int(np.sqrt(X_max + 3)) + 1):
        if sieve[i]:
            sieve[i*i::i] = False

    twins = []
    for p in range(3, X_max + 1, 2):
        if sieve[p] and sieve[p + 2]:
            twins.append(p)
    return np.array(twins)

def build_matrices(twins, t=1.0):
    """Build Q and G matrices"""
    N = len(twins)
    xi = np.log(twins) / (2 * np.pi)

    G = np.zeros((N, N))
    A = np.zeros((N, N))

    for i in range(N):
        for j in range(N):
            delta = xi[j] - xi[i]
            K = np.sqrt(2 * np.pi * t) * np.exp(-delta**2 / (4 * t))
            G[i, j] = K
            A[i, j] = delta * K

    Q = A.T @ A
    return Q, G, A, xi

def min_on_cone(Q, G, n_trials=200):
    """Find minimum of R on cone"""
    N = Q.shape[0]

    def R(lam):
        lam = np.maximum(lam, 1e-10)
        lam = lam / np.linalg.norm(lam)
        return (lam @ Q @ lam) / (lam @ G @ lam)

    best_R = np.inf
    for _ in range(n_trials):
        x0 = np.random.exponential(size=N)
        x0 = x0 / np.linalg.norm(x0)
        result = minimize(R, x0, method='L-BFGS-B', bounds=[(1e-10, None)] * N)
        if result.fun < best_R:
            best_R = result.fun

    return best_R

def lower_bound_attempt(Q, G):
    """
    Attempt to derive analytical lower bound on min_cone R.

    Key insight: For any Î» â‰¥ 0 with ||Î»|| = 1:

    R(Î») = Î»áµ€QÎ» / Î»áµ€GÎ»

    Lower bound approach 1: Trace inequality
    ------------------------------------
    For positive semidefinite Q:
    Î»áµ€QÎ» â‰¥ Î»_min(Q) Ã— ||Î»||Â² = Î»_min(Q)

    For positive definite G:
    Î»áµ€GÎ» â‰¤ Î»_max(G) Ã— ||Î»||Â² = Î»_max(G)

    So: R(Î») â‰¥ Î»_min(Q) / Î»_max(G)

    But this is TERRIBLE because Î»_min(Q) can be 0!

    Lower bound approach 2: Restricted eigenvalue
    -------------------------------------------
    On cone, we don't need global Î»_min(Q).
    We need: inf_{Î» â‰¥ 0, ||Î»||=1} Î»áµ€QÎ»

    This is the "cone-restricted minimum eigenvalue".
    """

    N = Q.shape[0]

    # Standard eigenvalues
    eigvals_Q = np.linalg.eigvalsh(Q)
    eigvals_G = np.linalg.eigvalsh(G)

    lambda_min_Q = np.min(eigvals_Q)
    lambda_max_G = np.max(eigvals_G)

    # Trivial bound
    trivial_bound = max(0, lambda_min_Q) / lambda_max_G

    # Diagonal bound: min_i Q_ii / max_i G_ii
    diag_bound = np.min(np.diag(Q)) / np.max(np.diag(G))

    # Row sum bound
    Q_rowsum = np.sum(Q, axis=1)
    G_rowsum = np.sum(G, axis=1)
    rowsum_bound = np.min(Q_rowsum) / np.max(G_rowsum)

    return {
        'trivial': trivial_bound,
        'diagonal': diag_bound,
        'rowsum': rowsum_bound,
        'lambda_min_Q': lambda_min_Q,
        'lambda_max_G': lambda_max_G
    }

def analyze_scaling():
    """Analyze scaling of min_R with N"""

    print("=" * 70)
    print("SCALING ANALYSIS: min_R vs N")
    print("=" * 70)

    X_values = [100, 200, 500, 1000, 2000, 5000, 10000, 20000]

    data = []
    for X in X_values:
        twins = get_twins(X)
        N = len(twins)
        if N < 5:
            continue

        Q, G, A, xi = build_matrices(twins)
        min_R = min_on_cone(Q, G)
        bounds = lower_bound_attempt(Q, G)

        # Also compute key quantities
        Tr_Q = np.trace(Q)
        Tr_G = np.trace(G)
        Sum_Q = np.sum(Q)
        Sum_G = np.sum(G)

        data.append({
            'X': X, 'N': N,
            'min_R': min_R,
            'Tr_Q_Tr_G': Tr_Q / Tr_G,
            'Sum_Q_Sum_G': Sum_Q / Sum_G,
            'bounds': bounds
        })

        print(f"X = {X:5d}, N = {N:3d}: min_R = {min_R:.4f}, "
              f"diag_bound = {bounds['diagonal']:.4f}, "
              f"rowsum_bound = {bounds['rowsum']:.4f}")

    # Fit power law
    N_arr = np.array([d['N'] for d in data])
    min_R_arr = np.array([d['min_R'] for d in data])

    log_N = np.log(N_arr)
    log_min_R = np.log(min_R_arr)

    # Linear fit in log-log
    coeffs = np.polyfit(log_N, log_min_R, 1)
    exponent = coeffs[0]
    prefactor = np.exp(coeffs[1])

    print(f"\nPOWER LAW FIT: min_R â‰ˆ {prefactor:.4f} Ã— N^{exponent:.4f}")

    # Check bounds scaling
    diag_bounds = np.array([d['bounds']['diagonal'] for d in data])
    rowsum_bounds = np.array([d['bounds']['rowsum'] for d in data])

    print("\nBOUNDS ANALYSIS:")
    print(f"  Diagonal bounds: {diag_bounds}")
    print(f"  Rowsum bounds:   {rowsum_bounds}")

    # Can we prove min_R â‰¥ c Ã— N^Î´?
    print("\n" + "=" * 70)
    print("KEY QUESTION: Can we prove min_R â‰¥ c Ã— N^Î´?")
    print("=" * 70)

    # Check ratio min_R / N^Î´
    delta_candidates = [0.5, 0.8, 1.0, 1.1]
    for delta in delta_candidates:
        ratios = min_R_arr / (N_arr ** delta)
        print(f"  Î´ = {delta:.1f}: min_R / N^Î´ = {ratios} (stable if Î´ correct)")

    return data, exponent

def prove_cone_restricted_bound():
    """
    Key theoretical result to prove:

    THEOREM (Cone-Restricted Rayleigh Bound):
    Let Q = Aáµ€A where A has "gradient structure" A_ij = (Î¾_j - Î¾_i) K_ij with K > 0.
    Let G be a Gram matrix with G_ii = Îº (constant diagonal).

    Then for any Î» â‰¥ 0 with ||Î»|| = 1:

    R(Î») = Î»áµ€QÎ» / Î»áµ€GÎ» â‰¥ ???

    Approach: Use structure of A.
    """

    print("\n" + "=" * 70)
    print("THEORETICAL ANALYSIS")
    print("=" * 70)

    print("""
KEY INSIGHT: A has "gradient structure"

A_ij = (Î¾_j - Î¾_i) Ã— K_ij

For any vector v:
(Av)_i = Î£_j (Î¾_j - Î¾_i) K_ij v_j
       = Î£_j Î¾_j K_ij v_j - Î¾_i Î£_j K_ij v_j
       = (KÎ¾v)_i - Î¾_i (Kv)_i

In matrix form: Av = K(Î¾Â·v) - Î¾Â·(Kv)
              = K diag(Î¾) v - diag(Î¾) K v
              = [K, diag(Î¾)] v      (commutator!)

So: A = [K, diag(Î¾)] = KÂ·diag(Î¾) - diag(Î¾)Â·K

This is the COMMUTATOR of K with the position operator!

Q = Aáµ€A = [K, diag(Î¾)]áµ€ [K, diag(Î¾)]

For Î» â‰¥ 0:
Î»áµ€QÎ» = ||AÎ»||Â² = ||[K, diag(Î¾)] Î»||Â²

QUESTION: Can we bound ||[K, diag(Î¾)] Î»||Â² from below for Î» â‰¥ 0?

Answer depends on:
1. Spread of Î¾ values (determines commutator size)
2. Structure of K (Gaussian, decays with distance)
3. Cone constraint (Î» â‰¥ 0)
    """)

def analyze_commutator_structure():
    """Analyze the commutator structure more deeply"""

    print("\n" + "=" * 70)
    print("COMMUTATOR STRUCTURE ANALYSIS")
    print("=" * 70)

    twins = get_twins(5000)
    N = len(twins)
    Q, G, A, xi = build_matrices(twins)

    # A = K @ diag(xi) - diag(xi) @ K
    # Let's verify this

    K = G  # K = G in our notation

    Xi = np.diag(xi)
    A_commutator = K @ Xi - Xi @ K

    # Check if A = A_commutator
    diff = np.max(np.abs(A - A_commutator))
    print(f"||A - [K, Xi]||_max = {diff:.2e} (should be ~0)")

    # Now, ||AÎ»||Â² = Î»áµ€[K,Xi]áµ€[K,Xi]Î»

    # For Î» â‰¥ 0, can we bound this?

    # Key observation: [K, Xi] = K Xi - Xi K
    # (K Xi)_ij = K_ij Î¾_j
    # (Xi K)_ij = Î¾_i K_ij
    # So [K, Xi]_ij = K_ij (Î¾_j - Î¾_i)

    # ||[K,Xi]Î»||Â² = Î£_i (Î£_j K_ij (Î¾_j - Î¾_i) Î»_j)Â²

    # For Î» = uniform = (1,...,1)/âˆšN:
    # (AÎ»)_i = (1/âˆšN) Î£_j K_ij (Î¾_j - Î¾_i)
    #        = (1/âˆšN) [Î£_j K_ij Î¾_j - Î¾_i Î£_j K_ij]
    #        = (1/âˆšN) [(KÎ¾)_i - Î¾_i (K1)_i]

    uniform = np.ones(N) / np.sqrt(N)
    A_uniform = A @ uniform

    # Compute (KÎ¾)_i and (K1)_i
    K_xi = K @ xi
    K_one = K @ np.ones(N)

    A_uniform_check = (1/np.sqrt(N)) * (K_xi - xi * K_one)
    diff2 = np.max(np.abs(A_uniform - A_uniform_check))
    print(f"||AÎ»_uniform - formula||_max = {diff2:.2e} (should be ~0)")

    # So ||AÎ»_uniform||Â² = (1/N) Î£_i [(KÎ¾)_i - Î¾_i (K1)_i]Â²

    numerator = np.sum(A_uniform**2)
    print(f"\n||AÂ·uniform||Â² = {numerator:.4f}")
    print(f"This equals Sum(Q)/N = {np.sum(Q)/N:.4f}")

    # Can we bound (KÎ¾)_i - Î¾_i (K1)_i from below?

    # (KÎ¾)_i = Î£_j K_ij Î¾_j = "weighted average of Î¾ around i"
    # Î¾_i (K1)_i = Î¾_i Ã— Î£_j K_ij = "Î¾_i times total weight"

    # If K is localized (Gaussian), (KÎ¾)_i â‰ˆ Î¾_i + correction
    # The correction comes from the asymmetry in twin distribution!

    print("\n" + "-" * 50)
    print("ASYMMETRY ANALYSIS")
    print("-" * 50)

    # For each i, compute "left weight" vs "right weight"
    left_weight = np.zeros(N)
    right_weight = np.zeros(N)
    left_xi_weight = np.zeros(N)
    right_xi_weight = np.zeros(N)

    for i in range(N):
        for j in range(N):
            if j < i:
                left_weight[i] += K[i, j]
                left_xi_weight[i] += K[i, j] * xi[j]
            elif j > i:
                right_weight[i] += K[i, j]
                right_xi_weight[i] += K[i, j] * xi[j]

    # (KÎ¾)_i - Î¾_i(K1)_i = left_xi + right_xi + K_ii Î¾_i - Î¾_i(left + right + K_ii)
    #                    = (left_xi - Î¾_i left) + (right_xi - Î¾_i right)

    # For uniform distribution:
    # left_xi â‰ˆ Î¾_i left (because average Î¾ on left â‰ˆ Î¾_i)
    # But for TWINS, distribution is NOT uniform!

    # At boundaries (i near 0 or near N), there's strong asymmetry

    print(f"Boundary asymmetry (i=0):   left_weight = {left_weight[0]:.4f}, right_weight = {right_weight[0]:.4f}")
    print(f"Boundary asymmetry (i=N-1): left_weight = {left_weight[-1]:.4f}, right_weight = {right_weight[-1]:.4f}")
    print(f"Middle (i=N/2):             left_weight = {left_weight[N//2]:.4f}, right_weight = {right_weight[N//2]:.4f}")

    # Contribution to ||AÎ»||Â² from boundaries
    boundary_contrib = A_uniform[0]**2 + A_uniform[-1]**2
    total = np.sum(A_uniform**2)
    print(f"\nBoundary contribution: {boundary_contrib:.4f} ({100*boundary_contrib/total:.1f}% of total)")

    # Key insight: boundaries give O(1) contribution each
    # There are ~âˆšN "boundary-like" points (where distribution is asymmetric)
    # Total contribution ~ âˆšN Ã— O(1) = O(âˆšN) ???

    # But numerically we see min_R ~ N^1.09, not N^0.5

    # Let's check the contribution from ALL points
    print("\nContribution by region:")
    regions = [
        (0, N//10, "first 10%"),
        (N//10, 9*N//10, "middle 80%"),
        (9*N//10, N, "last 10%")
    ]

    for start, end, name in regions:
        contrib = np.sum(A_uniform[start:end]**2)
        print(f"  {name}: {contrib:.4f} ({100*contrib/total:.1f}%)")

def main():
    print("ðŸ”¥ PROVING min_R ~ N^Î´ ðŸ”¥\n")

    data, exponent = analyze_scaling()

    prove_cone_restricted_bound()

    analyze_commutator_structure()

    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"""
NUMERICAL FINDING: min_R ~ N^{exponent:.2f}

THEORETICAL STRUCTURE:
- A = [K, diag(Î¾)] is the COMMUTATOR of kernel with position
- ||AÎ»||Â² measures "spectral displacement" of Î»
- For Î» â‰¥ 0, this cannot vanish (Cone-Kernel separation)
- Growth comes from: boundary effects + asymmetry in twin distribution

KEY THEOREM TO PROVE:
For A = [K, diag(Î¾)] with K Gaussian kernel and Î¾ strictly increasing:
  inf_{{Î» â‰¥ 0, ||Î»||=1}} ||AÎ»||Â² â‰¥ c Ã— N^Î´ for some c, Î´ > 0

This would complete the proof of TPC!
    """)

if __name__ == "__main__":
    main()
